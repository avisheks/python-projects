#!/bin/bash

HADOOP_PREFIX=/home/gs/hadoop/current
HADOOP=$HADOOP_PREFIX/bin/hadoop
HDFS=$HADOOP_PREFIX/bin/hadoop

WORK=/user/$USER/python-hadoop-streaming
DATA=/user/$USER/python-hadoop-streaming/data

set -x

run_cmd () {
    cmd=$1
    remove_outputs=$2
    echo "cmd=[$cmd]"
    eval $cmd
}

train_cmd_st="$HADOOP jar $HADOOP_PREFIX/share/hadoop/tools/lib/hadoop-streaming.jar \
    -Ddfs.umaskmode=022 \
    -Dmapreduce.job.queuename=$job_queue \
    -Dmapreduce.job.name=$task_name \
    -Dmapreduce.job.reduces=$num_models \
    -Dmapreduce.reduce.memory.mb=8000 \
    -Dmapreduce.task.timeout=60000000 \
    -Dstream.num.map.output.key.fields=2 \
    -Dnum.key.fields.for.partition=1 \
    -file mapper.py \
    -file reducer.py \
    -input $DATA/*.txt \
    -output $WORK/log \
    -mapper mapper.py \
    -reducer reducer.py \
run_cmd "$train_cmd_st" "$WORK"                                
